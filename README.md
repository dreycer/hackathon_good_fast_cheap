

# Optimizing a Model with a small sample

I took part in a four hour group hackathon where we were attempting to optimize given a particular constraint. My team's constraint was to have a much smaller sample than other groups. (Think 6500 data points versus 30,000).

![](https://berkonomics.com/wp-content/uploads/2015/11/goodfastcheap1-1.png)

The idea is that for any project you can have any two of these. You can have good work done cheap, but it will take a long time. You can have good work done fast, but it won't be cheap. Or you can have work done fast and on the cheap, but it won't be good.

We applied this concept to data science.

We were 'Team Sample'

---

### Team Sample Constraint
- Your choice of algorithm
- Your choice of features
- **Must use the cheap train sample**

### Team Features Constraint
- Your choice of algorithm
- **Limited to a maximum of 20 features**
- Your choice of samples

### Team Algorithm Constraint
- **Must use a Random Forest**
- Your choice of features
- Your choice of samples

[Hackathon Code](https://git.generalassemb.ly/dreycer/Hackathon-Good-Fast-Cheap/blob/student_facing/Hackathon%20Code.ipynb) 

 Descriptions of the data can be found [here](https://archive.ics.uci.edu/ml/datasets/adult). 
 
